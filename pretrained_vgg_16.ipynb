{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## No se si hace falta para el dataset final -> tengo dos archivos csv porque me he descargado por separado las img benignas y las malignas -> concateno los csv !!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# Cargar los datos de los dos archivos CSV\n",
    "data_1 = pd.read_csv('dataset_nuria/metadata_1.csv')\n",
    "data_2 = pd.read_csv('dataset_nuria/metadata_2.csv')\n",
    "\n",
    "# Combinar los dos DataFrames\n",
    "combined_data = pd.concat([data_1, data_2], ignore_index=True)\n",
    "\n",
    "# Guardar el DataFrame combinado en un nuevo archivo CSV\n",
    "combined_data.to_csv('dataset_nuria/metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             x_col   y_col\n",
      "0   dataset_nuria/ISIC_0085644.jpg  benign\n",
      "1   dataset_nuria/ISIC_0260516.jpg  benign\n",
      "2   dataset_nuria/ISIC_0296251.jpg  benign\n",
      "3   dataset_nuria/ISIC_0378777.jpg  benign\n",
      "4   dataset_nuria/ISIC_0438180.jpg  benign\n",
      "..                             ...     ...\n",
      "95  dataset_nuria/ISIC_8651648.jpg  malign\n",
      "96  dataset_nuria/ISIC_9098311.jpg  malign\n",
      "97  dataset_nuria/ISIC_9320992.jpg  malign\n",
      "98  dataset_nuria/ISIC_9677008.jpg  malign\n",
      "99  dataset_nuria/ISIC_9928278.jpg  malign\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "## Crear el DataFrame necesario para ImageDataGenerator\n",
    "dataset_path = 'dataset_nuria/' # Cambiar tuta del archivo CSV !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# Cargar el archivo con las etiquetas\n",
    "data = pd.read_csv(dataset_path + 'metadata.csv') \n",
    "\n",
    "# Crear el DataFrame con dos columnas -> ruta de la imagen | etiqueta (0:benigno, 1:melanoma) \n",
    "new_data = pd.DataFrame({'x_col': dataset_path + data['isic_id'] + '.jpg',\n",
    "                        'y_col': data['benign_malignant'].apply(lambda x: 'benign' if x == 'benign' else 'malign')})\n",
    "\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80 validated image filenames belonging to 2 classes.\n",
      "Found 20 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Crear el generador de datos de imagenes en tiempo real -> Aplicar una normalizacion + Separacion Train Test (20% Train y 80% Test)\n",
    "datagen = ImageDataGenerator(rescale=1./255., validation_split=0.20)\n",
    "\n",
    "batch_size = 20 # Lo ponen a 128 pero con 100 imagenes no puedo ponerlo a tanto!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# Generador de datos de entrenamiento\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=new_data,\n",
    "    x_col='x_col',\n",
    "    y_col='y_col',\n",
    "    target_size=(224,224),\n",
    "    color_mode='rgb',\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Generador de datos de validacion\n",
    "test_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=new_data,\n",
    "    x_col='x_col',\n",
    "    y_col='y_col',\n",
    "    target_size=(224,224),\n",
    "    color_mode='rgb',\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo VGG 16 preentrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nuria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4096)              102764544 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 4097      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134264641 (512.18 MB)\n",
      "Trainable params: 119549953 (456.05 MB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo VGG16 preentrenado\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Congelar las capas del modelo base\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Añadir capas para la clasificacion binaria\n",
    "x = base_model.output\n",
    "flat   = Flatten()(x)\n",
    "dense1 = Dense(4096, activation=\"relu\")(flat)\n",
    "drop1  = Dropout(0.5)(dense1)\n",
    "dense2 = Dense(4096, activation=\"relu\")(drop1)\n",
    "drop2  = Dropout(0.5)(dense2)\n",
    "output = Dense(1, activation=\"sigmoid\")(drop2)  # Clasificacion binaria\n",
    "\n",
    "# Modelo\n",
    "vgg_16_model = Model(inputs=base_model.input, outputs=output)\n",
    "vgg_16_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "vgg_16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:From c:\\Users\\nuria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\nuria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "4/4 [==============================] - ETA: 0s - loss: 21.2122 - accuracy: 0.5000 \n",
      "Epoch 1: saving model to pretrained_modelVVG.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nuria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 125s 31s/step - loss: 21.2122 - accuracy: 0.5000 - val_loss: 2.1727 - val_accuracy: 0.5000\n",
      "Epoch 2/30\n",
      "4/4 [==============================] - ETA: 0s - loss: 6.1367 - accuracy: 0.5000 \n",
      "Epoch 2: saving model to pretrained_modelVVG.h5\n",
      "4/4 [==============================] - 105s 27s/step - loss: 6.1367 - accuracy: 0.5000 - val_loss: 1.0052 - val_accuracy: 0.5000\n",
      "Epoch 3/30\n",
      "3/4 [=====================>........] - ETA: 15s - loss: 1.3004 - accuracy: 0.5000"
     ]
    }
   ],
   "source": [
    "# Detiene el entrenamiento si la precision de la validacion no mejora en despues de 2 epocas\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=2, verbose=1)\n",
    "\n",
    "# Guarda el modelo con mejor precision en la validacion\n",
    "mcp = ModelCheckpoint('pretrained_modelVVG.h5', verbose=1)\n",
    "\n",
    "# Parametros !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "train_steps = train_generator.n // train_generator.batch_size\n",
    "test_steps = test_generator.n // test_generator.batch_size\n",
    "epochs = 30\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "history = vgg_16_model.fit(train_generator, steps_per_epoch=train_steps, epochs=epochs, validation_data=test_generator, validation_steps=test_steps, verbose=1, callbacks=[mcp,early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graficos del Accuracy y la perdida\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_accuracy, label='Train Accuracy')\n",
    "plt.plot(val_accuracy, label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Perdida\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cargar el modelo -> No se si sera necesario !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "model = load_model('./pretrained_modelVVG.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Matriz de confusion\n",
    "\n",
    "# Predicciones para el conjunto de datos de test\n",
    "predictions = model.predict(test_generator, verbose=1) # cambiar por vgg_16_model.predict !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "predictions = predictions.argmax(axis=1)\n",
    "print('Preditions: \\n', predictions)\n",
    "\n",
    "# Etiquetas del conjunto de datos de test (GT)\n",
    "test_labels = test_generator.classes\n",
    "print('Ground Truth: \\n', test_labels)\n",
    "\n",
    "# Matriz de confusión\n",
    "matrix = confusion_matrix(test_labels, predictions)\n",
    "print('Confusion matrix: \\n', matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_generator.classes, predictions, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
