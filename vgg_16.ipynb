{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## No se si hace falta para el dataset final -> tengo dos archivos csv porque me he descargado por separado las img benignas y las malignas -> concateno los csv !!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# Cargar los datos de los dos archivos CSV\n",
    "data_1 = pd.read_csv('dataset_nuria/metadata_1.csv')\n",
    "data_2 = pd.read_csv('dataset_nuria/metadata_2.csv')\n",
    "\n",
    "# Combinar los dos DataFrames\n",
    "combined_data = pd.concat([data_1, data_2], ignore_index=True)\n",
    "\n",
    "# Guardar el DataFrame combinado en un nuevo archivo CSV\n",
    "combined_data.to_csv('dataset_nuria/metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             x_col   y_col\n",
      "0   dataset_nuria/ISIC_0085644.jpg  benign\n",
      "1   dataset_nuria/ISIC_0260516.jpg  benign\n",
      "2   dataset_nuria/ISIC_0296251.jpg  benign\n",
      "3   dataset_nuria/ISIC_0378777.jpg  benign\n",
      "4   dataset_nuria/ISIC_0438180.jpg  benign\n",
      "..                             ...     ...\n",
      "95  dataset_nuria/ISIC_8651648.jpg  malign\n",
      "96  dataset_nuria/ISIC_9098311.jpg  malign\n",
      "97  dataset_nuria/ISIC_9320992.jpg  malign\n",
      "98  dataset_nuria/ISIC_9677008.jpg  malign\n",
      "99  dataset_nuria/ISIC_9928278.jpg  malign\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "## Crear el DataFrame necesario para ImageDataGenerator\n",
    "dataset_path = 'dataset_nuria/' # Cambiar tuta del archivo CSV !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# Cargar el archivo con las etiquetas\n",
    "data = pd.read_csv(dataset_path + 'metadata.csv') \n",
    "\n",
    "# Crear el DataFrame con dos columnas -> ruta de la imagen | etiqueta (0:benigno, 1:melanoma) \n",
    "new_data = pd.DataFrame({'x_col': dataset_path + data['isic_id'] + '.jpg',\n",
    "                        'y_col': data['benign_malignant'].apply(lambda x: 'benign' if x == 'benign' else 'malign')}) # melanoma\n",
    "\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80 validated image filenames belonging to 2 classes.\n",
      "Found 20 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Crear el generador de datos de imagenes en tiempo real -> Aplicar una normalizacion + Separacion Train Test (20% Train y 80% Test)\n",
    "datagen = ImageDataGenerator(rescale=1./255., validation_split=0.20)\n",
    "\n",
    "batch_size = 20 # Lo ponen a 128 pero con 100 imagenes no puedo ponerlo a tanto!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# Generador de datos de entrenamiento\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=new_data,\n",
    "    x_col='x_col',\n",
    "    y_col='y_col',\n",
    "    target_size=(224,224),\n",
    "    color_mode='rgb',\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Generador de datos de validacion\n",
    "test_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=new_data,\n",
    "    x_col='x_col',\n",
    "    y_col='y_col',\n",
    "    target_size=(224,224),\n",
    "    color_mode='rgb',\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_65 (Conv2D)          (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " conv2d_66 (Conv2D)          (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " max_pooling2d_25 (MaxPooli  (None, 112, 112, 64)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_67 (Conv2D)          (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " conv2d_68 (Conv2D)          (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " max_pooling2d_26 (MaxPooli  (None, 56, 56, 128)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_69 (Conv2D)          (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " conv2d_70 (Conv2D)          (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " conv2d_71 (Conv2D)          (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " max_pooling2d_27 (MaxPooli  (None, 28, 28, 256)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_72 (Conv2D)          (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " conv2d_73 (Conv2D)          (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " conv2d_74 (Conv2D)          (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " max_pooling2d_28 (MaxPooli  (None, 14, 14, 512)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_75 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " conv2d_76 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " conv2d_77 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " max_pooling2d_29 (MaxPooli  (None, 7, 7, 512)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 4096)              102764544 \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 4097      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134264641 (512.18 MB)\n",
      "Trainable params: 134264641 (512.18 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Dimensiones de las imagenes\n",
    "input = Input((224, 224, 3)) \n",
    "\n",
    "## Capas Convolucionales\n",
    "# Bloque 1\n",
    "conv1  = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(input)\n",
    "conv2  = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv1)\n",
    "pool1  = MaxPooling2D((2, 2))(conv2)\n",
    "\n",
    "# Bloque 2\n",
    "conv3  = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool1)\n",
    "conv4  = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv3)\n",
    "pool2  = MaxPooling2D((2, 2))(conv4)\n",
    "\n",
    "# Bloque 3\n",
    "conv5  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool2)\n",
    "conv6  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv5)\n",
    "conv7  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv6)\n",
    "pool3  = MaxPooling2D((2, 2))(conv7)\n",
    "\n",
    "# Bloque 4\n",
    "conv8  = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool3)\n",
    "conv9  = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv8)\n",
    "conv10 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv9)\n",
    "pool4  = MaxPooling2D((2, 2))(conv10)\n",
    "\n",
    "# Bloque 5\n",
    "conv11 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool4)\n",
    "conv12 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv11)\n",
    "conv13 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv12)\n",
    "pool5  = MaxPooling2D((2, 2))(conv13)\n",
    "\n",
    "## Capas Densas\n",
    "flat   = Flatten()(pool5)\n",
    "dense1 = Dense(4096, activation=\"relu\")(flat)\n",
    "drop1  = Dropout(0.5)(dense1)\n",
    "dense2 = Dense(4096, activation=\"relu\")(drop1)\n",
    "drop2  = Dropout(0.5)(dense2)\n",
    "output = Dense(1, activation=\"sigmoid\")(drop2)  # Clasificacion binaria\n",
    "\n",
    "# Modelo\n",
    "vgg_16_model = Model(inputs=input, outputs=output)\n",
    "vgg_16_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "vgg_16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.4176 - accuracy: 0.6500 \n",
      "Epoch 1: saving model to modelVVG.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nuria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 190s 46s/step - loss: 1.4176 - accuracy: 0.6500 - val_loss: 1.2223 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6711 - accuracy: 0.6250 \n",
      "Epoch 2: saving model to modelVVG.h5\n",
      "4/4 [==============================] - 183s 47s/step - loss: 0.6711 - accuracy: 0.6250 - val_loss: 0.8285 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/30\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6719 - accuracy: 0.6250 \n",
      "Epoch 3: saving model to modelVVG.h5\n",
      "4/4 [==============================] - 168s 41s/step - loss: 0.6719 - accuracy: 0.6250 - val_loss: 0.8794 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/30\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6580 - accuracy: 0.6250 \n",
      "Epoch 4: saving model to modelVVG.h5\n",
      "4/4 [==============================] - 169s 42s/step - loss: 0.6580 - accuracy: 0.6250 - val_loss: 1.1009 - val_accuracy: 0.0000e+00\n",
      "Epoch 4: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Detiene el entrenamiento si la precision de la validacion no mejora en despues de 2 epocas\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=2, verbose=1)\n",
    "\n",
    "# Guarda el modelo con mejor precision en la validacion\n",
    "mcp = ModelCheckpoint('modelVVG.h5', verbose=1)\n",
    "\n",
    "# Parametros !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "train_steps = train_generator.n // train_generator.batch_size\n",
    "test_steps = test_generator.n // test_generator.batch_size\n",
    "epochs = 30\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "history = vgg_16_model.fit(train_generator, steps_per_epoch=train_steps, epochs=epochs, validation_data=test_generator, validation_steps=test_steps, verbose=1, callbacks=[mcp,early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Graficos del Accuracy y la perdida\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m val_accuracy \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "## Graficos del Accuracy y la perdida\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_accuracy, label='Train Accuracy')\n",
    "plt.plot(val_accuracy, label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Perdida\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nuria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\nuria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Cargar el modelo -> No se si sera necesario !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "model = load_model('./modelVVG.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 11s 11s/step\n",
      "Preditions: \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Ground Truth: \n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Confusion matrix: \n",
      " [[20]]\n"
     ]
    }
   ],
   "source": [
    "## Matriz de confusion\n",
    "\n",
    "# Predicciones para el conjunto de datos de test\n",
    "predictions = model.predict(test_generator, verbose=1) # cambiar por vgg_16_model.predict !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "predictions = predictions.argmax(axis=1)\n",
    "print('Preditions: \\n', predictions)\n",
    "\n",
    "# Etiquetas del conjunto de datos de test (GT)\n",
    "test_labels = test_generator.classes\n",
    "print('Ground Truth: \\n', test_labels)\n",
    "\n",
    "# Matriz de confusión\n",
    "matrix = confusion_matrix(test_labels, predictions)\n",
    "print('Confusion matrix: \\n', matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00        20\n",
      "   macro avg       1.00      1.00      1.00        20\n",
      "weighted avg       1.00      1.00      1.00        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_generator.classes, predictions, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
